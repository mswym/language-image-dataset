{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##**UTILS**\n",
    "###**ORDER STIMULIS BY LABELS**\n",
    "# ----- ORDERS STIMULIS A TO B BY THEIR PREDICTED LABEL (IN LABELS)\n",
    "# ----- RETURNS ORDERED STIMULIS AND A LIST OF INDEXES\n",
    "def order_stimulis_by_labels(labels, a, b):\n",
    "    ordered_stimulis = {}\n",
    "    for label in labels:\n",
    "        ordered_stimulis[label] = []\n",
    "        for i in range(a, b):\n",
    "            data = get_original_preds(i, original_predictions, display=False)\n",
    "            pred_super, pred_basic = data[\"Superordinate\"][\"Prediction\"], data[\"Basic\"][\"Prediction\"]\n",
    "            label_super, label_basic = super_labels[pred_super], basic_labels[pred_basic]\n",
    "            if label_super == label:\n",
    "                ordered_stimulis[label].append(i)\n",
    "            elif label_basic == label:\n",
    "                ordered_stimulis[label].append(i)\n",
    "\n",
    "    return ordered_stimulis\n",
    "\n",
    "\n",
    "def order_words_by_labels(labels, metric):\n",
    "    ordered_words = {}\n",
    "\n",
    "    features = model.encode_text(clip.tokenize(labels).to(device))\n",
    "    basic_features = model.encode_text(clip.tokenize(basic_labels).to(device))\n",
    "\n",
    "    for label in labels:\n",
    "        ordered_words[label] = []\n",
    "\n",
    "    for basic_label in basic_labels:\n",
    "\n",
    "        max_sim, closest_label = 0, \"\"\n",
    "        for label in labels:\n",
    "            sim = metric(basic_label, label)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                closest_label = label\n",
    "\n",
    "        ordered_words[closest_label].append(basic_label)\n",
    "\n",
    "    return ordered_words\n",
    "\n",
    "\n",
    "###**DISPLAY ORDERED STIMULIS**\n",
    "def get_ticks_and_labels(ordered_stimulis):\n",
    "    labels = list(ordered_stimulis.keys())\n",
    "\n",
    "    # GET DELIMITATION TICKS\n",
    "    ticks = []\n",
    "    for label in ordered_stimulis:\n",
    "        if len(ticks) > 0:\n",
    "            ticks.append(ticks[len(ticks) - 1] + len(ordered_stimulis[label]))\n",
    "        else:\n",
    "            ticks.append(0)\n",
    "\n",
    "            if len(ordered_stimulis[label]) - 1 > 0:\n",
    "                ticks.append(len(ordered_stimulis[label]) - 1)\n",
    "\n",
    "    # GET CENTER TICKS\n",
    "    centers = []\n",
    "    for i in range(len(ticks) - 1): centers.append(math.floor((ticks[i] + ticks[i + 1]) / 2))\n",
    "\n",
    "    if len(centers) == len(labels) - 1:\n",
    "        centers.append(centers[len(centers) - 1] + 1)\n",
    "\n",
    "    # SET LABELS\n",
    "    centerLabels = labels\n",
    "    tickLabels = [\"\"] * len(ticks)\n",
    "\n",
    "    # CORRECT CENTERS\n",
    "\n",
    "    remove_centers = []\n",
    "    for i in range(len(centers)):\n",
    "        if ticks[i] == centers[i]: remove_centers.append(i)\n",
    "\n",
    "    for i in range(len(centers) - 1, -1, -1):\n",
    "        if i in remove_centers:\n",
    "            centers.pop(i)\n",
    "            tickLabels[i] = centerLabels[i]\n",
    "            centerLabels.pop(i)\n",
    "\n",
    "    return ticks, centers, tickLabels, centerLabels\n",
    "\n",
    "\n",
    "def display_img_with_ordered_labels(img, ordered_labels_x, ordered_labels_y, colorbar=False, size=(5, 5), ratio=(1, 1),\n",
    "                                    min=0, max=1, xlabel=\"\", ylabel=\"\", title=\"\", title_cbar=\"\"):\n",
    "    x_ticks, x_centers, x_tickLabels, x_centerLabels = get_ticks_and_labels(ordered_labels_x)\n",
    "    if None is ordered_labels_y:\n",
    "        y_ticks, y_centers, y_tickLabels, y_centerLabels = x_ticks, x_centers, x_tickLabels, x_centerLabels\n",
    "    else:\n",
    "        y_ticks, y_centers, y_tickLabels, y_centerLabels = get_ticks_and_labels(ordered_labels_y)\n",
    "\n",
    "    # --- DISPLAY IMAGE ---\n",
    "    fig, ax = plt.subplots(1, 1, figsize=size)\n",
    "    hm = ax.imshow(img, cmap='Spectral', interpolation='nearest', vmin=min, vmax=max)\n",
    "    ax.set_aspect(float(ratio[0]) / float(ratio[1]))\n",
    "    if colorbar:\n",
    "        cbar = fig.colorbar(hm)\n",
    "        cbar.set_label(title_cbar, rotation=270, labelpad=30)\n",
    "\n",
    "    ax.set_xticks(x_centers, minor=True)\n",
    "    ax.set_xticklabels(x_centerLabels, minor=True, rotation=90)\n",
    "    ax.set_xticks(x_ticks, minor=False)\n",
    "    ax.set_xticklabels(x_tickLabels, minor=False, rotation=90)\n",
    "    ax.set_yticks(y_centers, minor=True)\n",
    "    ax.set_yticklabels(y_centerLabels, minor=True, rotation=0)\n",
    "    ax.set_yticks(y_ticks, minor=False)\n",
    "    ax.set_yticklabels(y_tickLabels, minor=False, rotation=0)\n",
    "    ax.tick_params(axis=u'both', which=u'minor', length=0)\n",
    "    ax.tick_params(axis=u'both', which=u'major', length=10)\n",
    "    ax.set_xlabel(xlabel, labelpad=30, fontsize=18)\n",
    "    ax.set_ylabel(ylabel, labelpad=30, fontsize=18)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "##**REPRESENTATIONS FOR FEEDFORWARD MODELS**\n",
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.ToTensor(), normalize])\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "vgg19_bn = models.vgg19_bn(pretrained=True)\n",
    "resnet152 = models.resnet18(pretrained=True)\n",
    "\n",
    "models = {\n",
    "    \"VGG19 - Batch Normalization\": vgg19_bn,\n",
    "    \"ResNet152\": resnet152\n",
    "}\n",
    "reps = None\n",
    "\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global reps\n",
    "    reps = input[0]\n",
    "\n",
    "\n",
    "images = torch.cat(\n",
    "    [preprocess(Image.fromarray(s_156['visual_stimuli156'][0][i][0])).unsqueeze(0) for i in range(156)]).to(device)\n",
    "\n",
    "\n",
    "def get_RDM(model, images):\n",
    "    if hasattr(model, 'classifier'):\n",
    "        hook = model.classifier[-1].register_forward_hook(hook_fn)\n",
    "    elif hasattr(model, 'fc'):\n",
    "        hook = model.fc.register_forward_hook(hook_fn)\n",
    "    else:\n",
    "        assert (False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(images)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for i in range(reps.size(0)):\n",
    "        similarities.append([])\n",
    "        for j in range(reps.size(0)):\n",
    "            similarities[len(similarities) - 1].append(torch.nn.CosineSimilarity(dim=0)(reps[i], reps[j]).item())\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "for key in models:\n",
    "    print(key)\n",
    "    similarities = get_RDM(models[key], images)\n",
    "    ordered_labels = {\"animal\": [None] * 28, \"plant\": [None] * 14, \"food\": [None] * 16, \"indoor\": [None] * 22,\n",
    "                      \"outdoor\": [None] * 20, \"human body\": [None] * 24, \"human face\": [None] * 32}\n",
    "    display_img_with_ordered_labels(similarities, ordered_labels, None, True, (8, 8))\n",
    "    plt.show()\n",
    "##**REPRESENTATIONS CLIP**\n",
    "similarities = []\n",
    "features = torch.empty(0, 512).to(device)\n",
    "\n",
    "dataset_size = 156\n",
    "batch_size = 32\n",
    "\n",
    "#images = torch.cat([preprocess(Image.fromarray(s_92['visual_stimuli'][0][i][5])).unsqueeze(0) for i in range(92)]).to(device)\n",
    "#images = torch.cat([preprocess(Image.fromarray(s_156['visual_stimuli156'][0][i][0])).unsqueeze(0) for i in range(156)]).to(device)\n",
    "\n",
    "print(images.size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = model.encode_image(images)\n",
    "\n",
    "    print(features.size())\n",
    "\n",
    "for i in range(features.size(0)):\n",
    "    similarities.append([])\n",
    "    for j in range(features.size(0)):\n",
    "        similarities[len(similarities) - 1].append(torch.nn.CosineSimilarity(dim=0)(features[i], features[j]).item())\n",
    "ordered_labels = {\"animal\": [None] * 28, \"plant\": [None] * 14, \"food\": [None] * 16, \"indoor\": [None] * 22,\n",
    "                  \"outdoor\": [None] * 20, \"human body\": [None] * 24, \"human face\": [None] * 32}\n",
    "display_img_with_ordered_labels(similarities, ordered_labels, None, True, (8, 8))\n",
    "data_path = path + \"/DATA/\" + model_name + \"_\" + \"context\" + str(contexts.index(context)) + \"_wordsAdd_preds.pt\"\n",
    "dataset_size = 156\n",
    "words = list(set(super_labels) | set(basic_labels))\n",
    "print(len(words))\n",
    "# ----- CHECKING IF DATA ALREADY EXISTS -------------------------------------------\n",
    "if os.path.exists(data_path):\n",
    "    wordsAdd_predictions = torch.load(data_path, map_location=device)\n",
    "else:\n",
    "    wordsAdd_predictions = {}\n",
    "start = len(wordsAdd_predictions.keys())  # Start at where we're at\n",
    "#word = 'electronic'\n",
    "#word = 'vehicle'\n",
    "#word = 'outdoor'\n",
    "#word = 'indoor'\n",
    "#word = 'accessory'\n",
    "#word = 'sports'\n",
    "#word = 'kitchen'\n",
    "#word = 'food'\n",
    "#word = 'furniture'\n",
    "#word = 'appliance'\n",
    "word = 'animal'\n",
    "#word = 'person'\n",
    "wordsAdd_predictions[word] = []\n",
    "clear_output()\n",
    "print(word)\n",
    "\n",
    "images = []\n",
    "for i in range(len(s_156['visual_stimuli156'][0])):\n",
    "    images.append(s_156['visual_stimuli156'][0][i][0])\n",
    "#images = get_stimulis(0,156,preprocess,word=None).to(device)\n",
    "images = get_stimulis(0, 156, preprocess, word=word).to(device)\n",
    "similarities = []\n",
    "features = torch.empty(0, 512).to(device)\n",
    "\n",
    "dataset_size = 156\n",
    "batch_size = 32\n",
    "\n",
    "print(images.size())\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = model.encode_image(images)\n",
    "\n",
    "    print(features.size())\n",
    "\n",
    "for i in range(features.size(0)):\n",
    "    similarities.append([])\n",
    "    for j in range(features.size(0)):\n",
    "        similarities[len(similarities) - 1].append(torch.nn.CosineSimilarity(dim=0)(features[i], features[j]).item())\n",
    "ordered_labels = {\"animal\": [None] * 28, \"plant\": [None] * 14, \"food\": [None] * 16, \"indoor\": [None] * 22,\n",
    "                  \"outdoor\": [None] * 20, \"human body\": [None] * 24, \"human face\": [None] * 32}\n",
    "display_img_with_ordered_labels(similarities, ordered_labels, None, True, (8, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}